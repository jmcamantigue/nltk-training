# -*- coding: utf-8 -*-
"""7-tensorflow-board.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jvG52g1Z0jfwC7m_MGwfughSilJVDxbW
"""

import tensorflow as tf
import numpy as np

from sklearn.datasets import load_iris

x = load_iris().data
y = load_iris().target

x[:10], y[:10]
np.unique(y)

from sklearn.model_selection import train_test_split
x_train, x_text, y_train, y_test = train_test_split(x, y, test_size=0.20)

from keras.utils import to_categorical
y = to_categorical(y)

with tf.name_scope('input'):
  input_features = tf.placeholder(dtype=tf.float32, shape=[None, x.shape[1]], name='input_features')
  input_labels = tf.placeholder(dtype=tf.float32, shape=[None, 3], name='input_labels')
  
with tf.name_scope('model'):
  weights = tf.Variable(tf.random_normal(shape=[x.shape[1], 3]), name='weights')
  biases = tf.Variable(tf.random_normal(shape=[3]))
  linear_model = tf.add(tf.matmul(input_features, weights), biases)
  predictions = tf.nn.softmax(linear_model)

with tf.name_scope('training_ops'):
  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(
    logits=linear_model, labels=input_labels))
  train_op = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss)
  tf.summary.scalar(name='loss', tensor=loss)
  
with tf.name_scope('metrics'):
  correct_prediction = tf.cast(tf.equal(tf.argmax(predictions, 1),
                                       tf.argmax(input_labels, 1)), tf.float32)
  accuracy_op = tf.reduce_mean(correct_prediction)
  tf.summary.scalar(name='accuracy', tensor=accuracy_op)

def next_batch(batch_size, features, labels):
  indices = np.arange(start=0, stop=features.shape[0])
  np.random.shuffle(indices)
  indices = indices[:batch_size]
  return features[indices], labels[indices]

!rm -rf logs/
!rm ngrok*
!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip
!unzip ngrok-stable-linux-amd64.zip

LOG_DIR = './logs'
get_ipython().system_raw(
    'tensorboard --logdir {0} --host 0.0.0.0 --port 6006 &'.format(LOG_DIR)
)

get_ipython().system_raw('./ngrok http 6006 &')

! curl -s http://localhost:4040/api/tunnels | python3 -c \
    "import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])"

init_op = tf.global_variables_initializer()
batch_size = 15

with tf.Session() as sess:
  
  sess.run(init_op)
  summary = tf.summary.merge_all()
  writer = tf.summary.FileWriter(logdir='./logs', graph=tf.get_default_graph())

  for epoch in range(10):
    for index in range(int(x.shape[0]/batch_size)):
      mini_batch_x, mini_batch_y= next_batch(batch_size=batch_size, features=x, labels=y)

      _, train_loss = sess.run([train_op, loss], feed_dict={input_features: mini_batch_x, input_labels: mini_batch_y})

    print('Epoch: {0}, loss : {1}'.format(epoch, train_loss))