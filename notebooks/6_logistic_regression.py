# -*- coding: utf-8 -*-
"""6-logistic-regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KDChPKetZdzZR-LvMAnFp9FHLUR_4AvC
"""

import tensorflow as tf
import numpy as np

from sklearn.datasets import load_iris

x = load_iris().data
y = load_iris().target

x[:10], y[:10]
np.unique(y)

# batch_size 4
input_features = tf.placeholder(dtype=tf.float32, shape=[None, 4])
# batch size 3
input_labels = tf.placeholder(dtype=tf.float32, shape=[None, 3])
# [4, 3]
weights = tf.Variable(tf.random_normal(shape=[4, 3]))
# [3]
biases = tf.Variable(tf.random_normal(shape=[3]))
# Construct model
pred = tf.nn.softmax(tf.matmul(input_features, weights) + biases) # Softmax
loss = tf.reduce_mean(-tf.reduce_sum(input_labels*tf.log(pred), reduction_indices=1))
train_op = tf.train.GradientDescentOptimizer(learning_rate=0.002).minimize(loss)

sess = tf.Session()

sess.run(tf.global_variables_initializer())

def next_batch(batch_size, features, labels):
  indices = np.arange(start=0, stop=features.shape[0])
  np.random.shuffle(indices)
  indices = indices[:batch_size]
  return features[indices], labels[indices]

from keras.utils import to_categorical
y = to_categorical(y)

batch_size = 15
for epoch in range(50):
  for index in range(int(x.shape[0]/batch_size)):
    mini_batch_x, mini_batch_y = next_batch(batch_size=batch_size, features=x, labels=y)
    
    _, train_loss = sess.run([train_op, loss], feed_dict={input_features: mini_batch_x, input_labels: mini_batch_y})
    
  print('Epoch: {0}, loss : {1}'.format(epoch, train_loss))